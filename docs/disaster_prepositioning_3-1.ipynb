{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing!\n",
    "\n",
    "Before we dive into modeling the ESUPS optimization problem, here is quick overview of some sub-notebooks provided:\n",
    "* 3.A Introduction to Modeling Optimization Problems\n",
    "* 3.B More on Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to the ESUPS Optimization Model\n",
    "\n",
    "We'll start with a simplified version of the model, and we'll build from there.\n",
    "\n",
    "Consider the scenario where we're expecting to have a specific disaster and want to transport the supplies from warehouses with predefined stock as quickly as possible to the disaster. Perhaps it's hurricane season or a volcano has been showing increasing activity. As we have seen in past problems, we have to set up the variables, constraints, and an objective function.\n",
    "\n",
    "First, we list the input data we have:\n",
    "- $I$ is the set of warehouses\n",
    "- $\\tau_i$ is the driving time to ship a single item from warehouse $i \\in I$ to the disaster\n",
    "- $b_i$ is the starting inventory (buckets) at each warehouse $i \\in I$\n",
    "- $d$ is the demand we need to satisfy, i.e., the number of items we need to ship to the disaster\n",
    "\n",
    "Then, let's define our decision variables, i.e. the things we can vary.\n",
    "- $y_i$ is the amount of supplies to send from warehouse $i \\in I$ to our disaster\n",
    "\n",
    "Now that we have described what we can change with our variables, we can figure out how to represent the objective function!\n",
    "$$\n",
    "\\min \\sum_{i \\in I} \\tau_{i} \\cdot y_i\n",
    "$$\n",
    "and the constraints are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{s.t.}  & \\sum_{i \\in I} & y_{i} & = d & & \\quad \\text{(total supplies sent must meet demand)}\\\\\n",
    "& & y_i       & \\leq b_i & \\forall i \\in I & \\quad \\text{(you can't send more than a warehouse has)}\\\\\n",
    " &\\text{} & y_{i} & \\geq 0 &\\forall i \\in I & \\quad \\text{(you can't send negative supplies)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Model\n",
    "So now let's code and solve this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import pandas as pd\n",
    "\n",
    "simple_Allocation = pd.read_csv(\"data/simple_Allocation.csv\")\n",
    "\n",
    "# Prep data\n",
    "BucketsNeeded = 13561\n",
    "n = len(simple_Allocation)\n",
    "t = simple_Allocation.drivingTime_hrs\n",
    "b = simple_Allocation.Buckets\n",
    "\n",
    "# Create model\n",
    "model = gp.Model(\"simple_Allocation\")\n",
    "\n",
    "# Add decision variables\n",
    "y = model.addVars(n, name=\"Warehouse_Allocation\")\n",
    "\n",
    "# Add constraint to meet demand\n",
    "model.addConstr(\n",
    "    gp.quicksum(y[i] for i in range(n)) == BucketsNeeded, name=\"Meet_Demand\"\n",
    ")\n",
    "\n",
    "# Add in warehouse_constraints\n",
    "for i in range(n):\n",
    "    model.addConstr(y[i] <= b[i], name=f\"warehouse_endowment_{i}\")\n",
    "\n",
    "# Note we don't have a constraint for y >= 0 since it's assumed in the variable definition\n",
    "# Add objective\n",
    "objective = gp.quicksum(t[i] * y[i] for i in range(n))\n",
    "model.setObjective(objective, GRB.MINIMIZE)\n",
    "\n",
    "# Fire up the solver!\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's Anaylize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_Allocation_sol = simple_Allocation.copy()\n",
    "simple_Allocation_sol[\"y\"] = model.getAttr(\"X\", y)\n",
    "simple_Allocation_sol.sort_values(\"drivingTime_hrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may have noticed that this feels like overkill. If we want to position supplies to respond to a known disaster, you might think that we should just put them as close as possible. It's an intuitive solution that can be solved with a simple greedy algorithm. But of course, life is never that simple. \n",
    "\n",
    "Now that we've got the initial problem outlined, let's start making it more realistic with two additions:\n",
    "1.\tInstead of preparing for only one disaster, let's prepare for all the disasters that might occur.\n",
    "2.\tInstead of being an omniscient observer, let's say we aren't sure where the next disaster will be\n",
    "\n",
    "Now, these assumptions might be intimidating at first. How do you rigorously model information we don't know? Isn't this the realm of predictive models?\n",
    "\n",
    "This reaction is totally normal and indeed we often use predictive modeling when our information is incomplete, however, remember that this problem isn't easily suited to ML techniques due to the low availability of data, small feature space, and large solution space. So how do we approach this with optimization? Well now that we know how supplies will be dispensed for a given warehouse allocation, we can leverage the small set of historical disasters we do have to ask which allocations would have saved/minimized the time taken to respond to all of the disasters. Then our optimization techniques will give us a small number that could feasibly satisfy this (where our constraints meet, see the introduction for more on this) and we can just iterate over them.\n",
    "\n",
    "So how do we go about it in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Including All Disasters\n",
    "\n",
    "Before we begin, let's explicitly define our new problem with the additional requirements outlined in the previous section so we're all on the same page. Our first step is to add in the fact that there are more disasters than just one. We can do that by including a variable index to denote which disaster we're talking about. \n",
    "\n",
    "Let $K$ be the set of disasters, and $k$ be the index for the disaster scenario at hand, i.e., a storm, earthquake, or epidemic.\n",
    "\n",
    "In scenario $k$, the time for a warehouse $i$ to send $y_i$ items is \n",
    "$$\\tau_{ik} \\cdot y^k_i$$\n",
    "\n",
    "Repeating for all warehouses gives us\n",
    "$$  \\sum_i \\tau_{ik} \\cdot y^k_i $$\n",
    "\n",
    "Repeating for all disasters gives us\n",
    "$$ \\sum_k \\sum_i \\tau_{ik} \\cdot y^k_i $$\n",
    "\n",
    "The question that ESUPS, and subsequently this model, set out to solve is: Where should disaster relief supplies be stored in a country to ensure it is as fast and cheap as possible to get them to people in need in the event of a disaster? \n",
    "\n",
    "By comparing the overall travel time for every single disaster that we have data on, we can make determinations on whether storing different quantities of materials in different warehouses would be better or worse given our historical data.\n",
    "\n",
    "Before we get to solving, let's add in one more factor. Different disasters occur at different rates. A landlocked nation may be less likely to experience a disaster caused by a tropical storm than an earthquake, so shouldn't we weigh the response time to earthquakes more than that of a storm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Accounting for Randomness\n",
    "\n",
    "One of the most difficult tasks that we can encounter in data science problems is randomness or as it's often called \"stochastic\" elements. This can be seen in all kinds of ways, but in our case study, we're going to look at how we account for not knowing which disaster will hit next or even several years in the future. The way we do this is to create a stochastic model, which may initially seem intimidating, but in discrete events like this, it's super easy.\n",
    "\n",
    "If you're familiar with expected value this will quickly make sense, but no need to have any prior experience! See the sub-notebook 3.B for a quick intro.\n",
    "The idea is that we weigh the outcome of the event (i.e. total travel time in this case) by the probability it occurs. So, if an earthquake is 3 times as likely as a flood, we would rewrite the equation as:\n",
    "\n",
    "$$0.75 \\cdot\\text{(travel time earthquake)} + 0.25 \\cdot\\text{(travel time flood)} $$\n",
    "\n",
    "#### The Model\n",
    "\n",
    "Now that we've had some background on expected value, let's return to our model! Remember, our goal is to account for the inherent randomness associated with different disasters occuring.\n",
    "\n",
    "Let $P^k$ be the probability of disaster $k$ and $t^k$ be the total travel time involved in disaster $k$ from the previous sections. Using our definition of expected value, this gives us\n",
    "\n",
    "$$ \\sum_k P^k \\cdot t^k$$\n",
    "\n",
    "Now you might notice that we have already written an equation for the total travel time for disaster $k$. Substituting this in, we get\n",
    "\n",
    "$$\\sum_k P^k \\sum_i \\tau_{ik} \\cdot y^k_i$$\n",
    "\n",
    "So, you can see, in our case turning this problem into a stochastic optimization problem that can account for probability only involves one more value per disaster. Our final task right now is to minimize the time required to get supplies to the disasters given how likely each disaster is to occur:\n",
    "\n",
    "$$\n",
    "\\min_y \\sum_k P^k \\sum_i \\tau_{ik} \\cdot y^k_i\n",
    "$$\n",
    "\n",
    "And we can see the constraints are almost unchanged:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{s.t.} \\quad \\sum_{i} y^k_{i} & = d^k & \\forall k & \\quad \\text{(total supplies sent must meet demand)}\\\\\n",
    "y^k_i & \\leq b_i & \\forall i \\in I,~ \\forall k \\in K & \\quad \\text{(you can't send more than a warehouse has)}\\\\\n",
    "y^k_{i} & \\geq 0 & \\forall i \\in I,~ \\forall k \\in K & \\quad \\text{(you can't send negative supplies)}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that we made an important assumption when defining the warehouse capacity constraints: We assumed that not all disasters happen at the same time, and so we have the full warehouse capacity available for each disaster.\n",
    "\n",
    "All the probabilities should sum up to one, i.e., $\\sum_k P^k = 1$. How do we calculate the probabilities though? Well, we have good long-term data on what disasters have affected which countries. For now, we can go through that data and calculate the probability of disaster $k$ by counting how many times it has occurred and dividing by the number of total disasters.\n",
    "\n",
    "You may have noticed that every single disaster will have the same probability, or in other words, this will produce a uniform distribution. As a result, $P^k$ can effectively be removed from our objective function, so our new objective will give the exact same results as our old one. We'll talk about ways to potentially address this later on in the notebooks as an extension, but for the moment, the purpose of adding this is to show how easily the equation can be modified to be stochastic and to get users used to the idea.\n",
    "So now that we have the new objective function, we can let the model go ahead and let Gurobi solve it for us!\n",
    "\n",
    "However, before we need to read in the data and prepare it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses = pd.read_csv(\"data/warehouses.csv\")\n",
    "warehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceMatrix_scenarios = pd.read_csv(\"data/distanceMatrix_scenarios.csv\")\n",
    "distanceMatrix_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters = pd.read_csv(\"data/disasters.csv\")\n",
    "# one bucket is needed for 2.5 people on average\n",
    "total_buckets = warehouses[\"Buckets\"].sum()\n",
    "disasters[\"demand\"] = (\n",
    "    (disasters[\"People Impacted\"] / 2.5).round().clip(upper=total_buckets).astype(int)\n",
    ")\n",
    "demand = disasters[\"demand\"].values\n",
    "probs = [1 / len(demand)] * len(demand)\n",
    "\n",
    "disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(demand)  # number of disasters/scenarios\n",
    "t = distanceMatrix_scenarios.set_index([\"scenario\", \"warehouse\"]).squeeze().to_dict()\n",
    "b = warehouses[\"Buckets\"][warehouses[\"Buckets\"] > 0].reset_index(drop=True)\n",
    "n = len(b)  # number of warehouses\n",
    "\n",
    "model_2 = gp.Model(\"multiple_disasters\")\n",
    "\n",
    "# Amount to take per warehouse\n",
    "y = model_2.addVars(t.keys(), name=\"Warehouse_Allocation\")\n",
    "\n",
    "# Add constraints to meet demand for each disaster scenario k\n",
    "for k in range(m):\n",
    "\n",
    "    # Demand constraints\n",
    "    model_2.addConstr(y.sum(k, \"*\") == demand[k], name=f\"Meet_Demand_K:{k}\")\n",
    "\n",
    "    # Warehouse constraints\n",
    "    for i in range(n):\n",
    "        model_2.addConstr(y[k, i] <= b[i], name=f\"warehouse_endowment_K:{k}_I:{i}\")\n",
    "\n",
    "# Objective function to minimize the weighted driving time over all scenarios\n",
    "objective = gp.quicksum(\n",
    "    probs[k] * gp.quicksum(t[k, i] * y[k, i] for i in range(n)) for k in range(m)\n",
    ")\n",
    "\n",
    "# Optimize model\n",
    "model_2.setObjective(objective, GRB.MINIMIZE)\n",
    "model_2.optimize()\n",
    "\n",
    "# Store results in dataframe a\n",
    "a = pd.DataFrame(\n",
    "    {\n",
    "        \"Warehouse allocation\": model_2.getAttr(\"VarName\", y),\n",
    "        \"Buckets\": model_2.getAttr(\"X\", y),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Science Extension\n",
    "\n",
    "We've seen how the objective function uses the uniform distribution to solve the problem. But what if knowing that climate change is increasingly energizing large storms, we decide the past hurricane impacts aren't representative of what's to come? In this section we want to prompt you to come up with predictive elements to improve our models. Feel free to use some of the ideas below or go in an entirely new direction!\n",
    "\n",
    "In this section, we encourage you to think creatively about enhancing predictive models for climate-related disasters. Consider how to incorporate novel data sources, feature engineering techniques, and model architectures to improve predictions. Below are some suggested approaches, but feel free to explore entirely new directions!\n",
    "\n",
    "#### Case Study Focus: Coastal Eastern African Nations\n",
    "\n",
    "Using the disaster impact data for coastal Eastern African nations, can you develop a model to predict how these impacts might escalate for Madagascar in the coming years? Consider not only the historical data but also factors such as changes in sea surface temperatures, shifting storm tracks, population growth along vulnerable coastlines, and evolving infrastructure resilience. Further, can you integrate this predictive model into an optimization framework to better allocate resources for disaster preparedness and response?\n",
    "\n",
    "Potential Approaches to Explore:\n",
    "- Comparing Time Series Models: Traditional statistical time series models like ARIMAX (AutoRegressive Integrated Moving Average with Explanatory Variables) are commonly used to predict future values based on past data. How do these models compare with more advanced Recurrent Neural Network (RNN)-based approaches like Long Short-Term Memory (LSTM) networks or Gated Recurrent Units (GRUs) in capturing long-term dependencies, especially under non-stationary conditions induced by climate change?\n",
    "\n",
    "- Incorporating Geospatial Data: Geospatial features, such as latitude, longitude, elevation, and proximity to bodies of water, can play a crucial role in predicting the impact of tropical storms. Can we encode geospatial information using techniques like convolutional neural networks (CNNs) for spatial feature extraction, or leverage more specialized models such as Geographical Weighted Regression (GWR) or Graph Neural Networks (GNNs) to account for spatial dependencies?\n",
    "\n",
    "- Incorporating Climate Change Projections: Beyond just historical data, consider how future climate projections can be integrated into the model. Can we use downscaled climate model outputs or ensemble approaches to account for different climate scenarios? How would these scenarios affect the frequency and intensity of tropical storms affecting coastal Eastern African nations?\n",
    "\n",
    "- Feature Engineering with Climate Indicators: Introduce climate change indicators as predictive features. For example, how do trends in sea surface temperatures (SSTs), El Niño-Southern Oscillation (ENSO) phases, or the Atlantic Multi-decadal Oscillation (AMO) correlate with storm intensification? Would incorporating these indicators as additional time series variables enhance predictive accuracy?\n",
    "\t\n",
    "- Hybrid and Ensemble Models: Can we leverage hybrid models that combine both statistical and deep learning approaches, or use ensemble methods that aggregate predictions from multiple models? For example, combining ARIMAX for short-term predictions with LSTM for capturing long-term trends may provide a more comprehensive forecasting tool.\n",
    "\n",
    "- Optimization Integration: Once a reliable predictive model is established, how can it be integrated into an optimization framework for resource allocation? For example, can we build an optimization model that minimizes both the cost of disaster preparedness and the potential loss from future storm impacts?\n",
    "\n",
    "- Model Explainability and Decision-Making: How can we ensure that the model is interpretable for decision-makers? Consider the use of techniques like SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-agnostic Explanations) to explain which factors contribute most to the model’s predictions, helping policymakers make informed decisions.\n",
    "\n",
    "By combining predictive analytics with optimization, we can not only forecast future disaster impacts but also develop actionable strategies for minimizing those impacts. The goal is to make our models both more accurate and more useful in real-world applications, driving better outcomes for communities at risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='End'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312 (Python 3.12.7)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
