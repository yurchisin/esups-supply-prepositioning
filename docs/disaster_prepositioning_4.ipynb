{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting our code snippets into production-level code, we have a few goals:\n",
    "1.\t**Readability**: Our code should be easy to understand and follow, even for someone who didn’t write it. Clear naming conventions, well-structured functions, and comments where necessary help ensure that others can read and maintain the code effectively.\n",
    "2.\t**Robustness**: The code needs to handle different edge cases and potential errors gracefully, without breaking. This means writing code that checks for invalid inputs and unexpected conditions, and includes error handling to keep the program running smoothly.\n",
    "3.\t**Testing**: Code should be tested thoroughly to ensure it works as expected in various scenarios. This involves writing test cases that cover different functionalities, both common and rare, to catch bugs early and maintain confidence in the code’s reliability.\n",
    "\n",
    "To achieve these goals, we need a solid design approach that provides structure and flexibility to our code. This is where Object-Oriented Programming (OOP) comes in. It’s one of the most popular frameworks, and you’ve likely worked with it in class or in other settings. OOP offers a way to organize code into logical units that make it more robust, and more testable, and easier to read. By structuring our code around objects that encapsulate data and functionality, we create a modular framework that simplifies complex data operations and supports future growth.\n",
    "\n",
    "OOP promotes modularity, reusability, and maintainability, allowing for a clearer separation of concerns and reducing the likelihood of errors. Additionally, OOP’s use of abstraction and inheritance helps create reusable components, ensuring consistent and efficient code across all parts of the project. What does this all actually look like in practice?\n",
    "\n",
    "Let’s start with a general flow diagram:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reader and Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage in the process is obviously cleaning and processing the data, and one of the interesting things that you may not have seen in your course work is the segmentation of the data reader class and the dataset class. Often in smaller projects, the reader functionality is added as a class method, many times within the instantiation call, which can make intuitive sense. However, there are a few key benefits to separating them out when we work with larger projects.\n",
    "\n",
    "1. Separation of Concerns: By creating separate classes, each class is responsible for a distinct function — the data reader class handles the loading and preprocessing of data from various sources, while the dataset class manages how the data is stored, accessed, and manipulated. This separation makes the codebase easier to understand and modify, as changes to how data is read do not impact how the data is organized or utilized downstream.\n",
    "\n",
    "\n",
    "2.\tReusability: When data reading and data management are isolated into their own classes, they can be reused across different projects or tasks without modification. For instance, the same data reader class can be employed to load data from multiple sources (like CSVs, databases, or APIs) while reusing the dataset class to provide a uniform interface for accessing and manipulating data, regardless of its origin.\n",
    "3.\tMaintainability: Segmenting these responsibilities simplifies debugging and testing. If there is an issue with data loading, the problem is likely within the data reader class, whereas issues with data handling can be traced back to the dataset class. This modularity reduces the risk of unintended side effects when making changes or improvements, since each class operates independently.\n",
    "4.\tFlexibility: A segmented approach allows developers to quickly adapt to changes in data sources or formats without needing to rewrite core data handling logic. The data reader class can be updated to handle new data formats or sources, while the dataset class remains consistent in providing a structured interface for data access and manipulation.\n",
    "5.  Testability: Perhaps one of the largest advantages is a layout that more easily lends itself to testing. If someone manually creates a test set, either class can be tested regardless of the other’s state, and where the errors lie is often much easier to figure out.\n",
    "\n",
    "\n",
    "As you’ll come to see, one of the core principles used to develop large projects like this is similar to the idea of a black box from machine learning. With multiple people working together, there is an emphasis on smoothly integrating everyone’s work.  This approach can allow a potentially difficult task (loading and preparing the data) to be distributed among developers without requiring everyone to understand all the intricacies of each class.\n",
    "\n",
    "\n",
    "Let's see how this has been implemented!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -q https://raw.githubusercontent.com/Gurobi/modeling-examples/refs/heads/master/optimization202/ESUPS_case_study/setup_imports.py\n",
    "from setup_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Country:\n",
    "    id: str = field(hash=True)\n",
    "    continent: str = field(repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Location:\n",
    "    id: str = field(hash=True)\n",
    "    address: str = field(repr=False)\n",
    "    country: Country = field(repr=False)\n",
    "    latitude: float = field(repr=False)\n",
    "    longitude: float = field(repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterType:\n",
    "    id: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterImpact:\n",
    "    id: str\n",
    "    disaster: \"Disaster\" = field(repr=False)\n",
    "    location: \"DisasterLocation\" = field(repr=False)\n",
    "    sub_location_nr: int = field(repr=False)\n",
    "    total_affected: int = field(repr=False)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.id\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterLocation(Location):\n",
    "    def __repr__(self):\n",
    "        return self.id\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Disaster:\n",
    "    id: str\n",
    "    type: DisasterType = field(repr=False)\n",
    "    day: int = field(repr=False)\n",
    "    month: int = field(repr=False)\n",
    "    year: int = field(repr=False)\n",
    "    impacted_locations: list[DisasterImpact] = field(hash=False, repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Depot(Location):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Item:\n",
    "    id: str = field(hash=True)\n",
    "    weight: float = field(repr=False)  # Metric tons\n",
    "    volume: float = field(repr=False)  # Cubic metres\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransportMode:\n",
    "    id: str = field(hash=True)\n",
    "    distance_method: str\n",
    "    big_m_cost_elim: float\n",
    "    max_driving_time_cut_above_hrs: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DistanceInfo:\n",
    "    distance: float  # Kilometres\n",
    "    time: float  # Hours\n",
    "    cost_per_ton: float  # USD\n",
    "\n",
    "DistanceMatrix = dict[Tuple[Location, Location, TransportMode], DistanceInfo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class encapsulates all data required for the analysis and optimization tasks. It includes lists of depots, disasters, and disaster_locations, and dictionaries for probabilities of disasters, inventory levels, and other critical data. The _zero_demand_threshold is a constant used internally to determine when demand should be considered negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Dataset:\n",
    "    depots: list[Depot]\n",
    "    disasters: list[Disaster]\n",
    "    disaster_locations: list[DisasterLocation]\n",
    "    probabilities: dict[Disaster, float]\n",
    "    items: list[Item]\n",
    "    transport_modes: list[TransportMode]\n",
    "    inventory: dict[Tuple[Depot, Item], int]\n",
    "    inventory_scenarios: dict[str, dict[Tuple[Depot, Item], int]]\n",
    "    distance: DistanceMatrix\n",
    "    people_affected: dict[Tuple[DisasterImpact, Item], float]\n",
    "    persons_per_item_general: dict[Tuple[DisasterImpact, Item], float]\n",
    "    persons_per_item_monthly: dict[Tuple[DisasterImpact, Item], float]\n",
    "    disaster_affected_totals: dict[str, int]\n",
    "\n",
    "    _zero_demand_threshold = 1e6\n",
    "\n",
    "\n",
    "    '''\n",
    "    The take_disaster_subset method allows us to focus our analysis on a specific \n",
    "    subset of disasters by providing a predicate function that filters disasters \n",
    "    based on certain criteria (e.g., date range, disaster type). It recalculates \n",
    "    probabilities to ensure they sum up to 1 after filtering. Related data such as \n",
    "    locations, distances, and affected populations are also filtered to maintain \n",
    "    consistency. If no filtering occurs (i.e., all disasters are included), the method \n",
    "    returns the original dataset to avoid unnecessary duplication.\n",
    "    '''\n",
    "\n",
    "    def take_disaster_subset(self, predicate: Callable[[Disaster], bool]) -> \"Dataset\":\n",
    "        \"\"\"\n",
    "        Generate a smaller dataset by only selecting a subset of the disasters with corresponding data\n",
    "        \"\"\"\n",
    "        disasters = list(filter(predicate, self.disasters))\n",
    "\n",
    "        if len(disasters) == len(self.disasters):\n",
    "            return self\n",
    "\n",
    "        total_probability = sum(self.probabilities[disaster] for disaster in disasters)\n",
    "        probabilities = {\n",
    "            disaster: self.probabilities[disaster] / total_probability\n",
    "            for disaster in disasters\n",
    "        }\n",
    "        locations = [\n",
    "            impact.location\n",
    "            for disaster in disasters\n",
    "            for impact in disaster.impacted_locations\n",
    "        ]\n",
    "        distance = {\n",
    "            (source, destination, mode): cell\n",
    "            for (source, destination, mode), cell in self.distance.items()\n",
    "            if destination in locations\n",
    "        }\n",
    "        people_affected = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.people_affected.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        persons_per_item_general = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.persons_per_item_general.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        persons_per_item_monthly = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.persons_per_item_monthly.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        return Dataset(\n",
    "            self.depots,\n",
    "            disasters,\n",
    "            locations,\n",
    "            probabilities,\n",
    "            self.items,\n",
    "            self.transport_modes,\n",
    "            self.inventory,\n",
    "            self.inventory_scenarios,\n",
    "            distance,\n",
    "            people_affected,\n",
    "            persons_per_item_general,\n",
    "            persons_per_item_monthly,\n",
    "            self.disaster_affected_totals,\n",
    "        )\n",
    "    \n",
    "    '''\n",
    "    The take_inventory_scenario method enables us to switch between different \n",
    "    inventory configurations stored in inventory_scenarios. By specifying a filename \n",
    "    key, we retrieve the corresponding inventory data. If the requested inventory \n",
    "    is already in use, the method returns the current dataset. Otherwise, it creates \n",
    "    a new Dataset instance with the updated inventory, facilitating comparative \n",
    "    analysis of different supply scenarios.\n",
    "    '''\n",
    "\n",
    "    def take_inventory_scenario(self, filename: str):\n",
    "        if filename not in self.inventory_scenarios:\n",
    "            raise RuntimeError(\"Inventory scenario not found\")\n",
    "        inventory = self.inventory_scenarios[filename]\n",
    "        if inventory == self.inventory:\n",
    "            return self\n",
    "        return Dataset(\n",
    "            self.depots,\n",
    "            self.disasters,\n",
    "            self.disaster_locations,\n",
    "            self.probabilities,\n",
    "            self.items,\n",
    "            self.transport_modes,\n",
    "            inventory,\n",
    "            self.inventory_scenarios,\n",
    "            self.distance,\n",
    "            self.people_affected,\n",
    "            self.persons_per_item_general,\n",
    "            self.persons_per_item_monthly,\n",
    "            self.disaster_affected_totals,\n",
    "        )\n",
    "    '''\n",
    "    The general_demand property calculates the overall demand for each \n",
    "    item at each impacted location using general beta values \n",
    "    (persons_per_item_general). The beta value represents how many people \n",
    "    can be served per item. We utilize the _calc_items_needed helper method \n",
    "    for calculations. The @functools.cached_property decorator ensures that \n",
    "    the result is computed once and cached, improving performance for repeated \n",
    "    access.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    @functools.cached_property\n",
    "    def general_demand(self) -> dict[Tuple[DisasterImpact, Item], float]:\n",
    "        general_demand = {\n",
    "            (location, item): self._calc_items_needed(\n",
    "                self.people_affected[location, item],\n",
    "                self.persons_per_item_general[location, item],\n",
    "            )\n",
    "            for disaster in self.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "            for item in self.items\n",
    "        }\n",
    "\n",
    "        return {key: value for key, value in general_demand.items() if value > 1e-1}\n",
    "\n",
    "\n",
    "    '''\n",
    "    Similarly, the monthly_demand property calculates demand on a monthly \n",
    "    basis using persons_per_item_monthly. This allows us to capture time-sensitive \n",
    "    demand variations, which are crucial in disaster response. The result is \n",
    "    cached, and we filter out values below 1e-3 to maintain computational efficiency.\n",
    "    '''\n",
    "\n",
    "    @functools.cached_property\n",
    "    def monthly_demand(self) -> dict[Tuple[DisasterImpact, Item], float]:\n",
    "        monthly_demand = {\n",
    "            (location, item): self._calc_items_needed(\n",
    "                self.people_affected[location, item],\n",
    "                self.persons_per_item_monthly[location, item],\n",
    "            )\n",
    "            for disaster in self.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "            for item in self.items\n",
    "        }\n",
    "\n",
    "        return {key: value for key, value in monthly_demand.items() if value > 1e-3}\n",
    "\n",
    "    '''\n",
    "    The _calc_items_needed helper method computes the number of items required \n",
    "    based on the number of people affected and the beta value. If beta is zero or \n",
    "    exceeds a predefined threshold (_zero_demand_threshold), indicating negligible \n",
    "    demand, the method returns zero. Otherwise, it divides people_affected by beta \n",
    "    to determine the required items. This calculation is fundamental for planning supply \n",
    "    quantities in logistics.\n",
    "    '''\n",
    "    \n",
    "    def _calc_items_needed(\n",
    "        self,\n",
    "        people_affected: float,\n",
    "        beta: float,\n",
    "    ):\n",
    "        if beta == 0 or beta >= self._zero_demand_threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return people_affected / beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from earlier, to solve for optimal allocations we have to consider several independent items, which requires solving the model for each version. This can get messy pretty quickly, considering the complexity of the model and the dataset that it uses. Therefore, in the actual implementation, the developers created a class to handle this larger loop. It will set up the problem for a specific context, and then call a dedicated solver class.\n",
    "\n",
    "While we won’t go into detail here, this setup also lends itself nicely to leveraging multithreading. You’ll notice that there is a sub-class in the code which creates “workers.” These allow users to handle each problem independently, and are a common set-up in python for multi-threading. Luckily for our sake, the problem is overall fast enough to be solved on a single instance, so this hasn’t been implemented fully in production code.\n",
    "\n",
    "Now let’s see how it all comes together. To start, we’ll define our name space and methods used from the data reader class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from multiprocessing import Pool\n",
    "from os import getenv\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.data import Dataset, Disaster, DisasterImpact, DistanceInfo, Item\n",
    "from src.solving import (\n",
    "    AllocationStrategy,\n",
    "    CostMatrix,\n",
    "    Problem,\n",
    "    Solution,\n",
    "    SolverParameters,\n",
    "    StochasticSolver,\n",
    ")\n",
    "\n",
    "class SolverObjective(IntEnum):\n",
    "    Cost = (0,)\n",
    "    Time = (1,)\n",
    "    Distance = 2\n",
    "\n",
    "SolutionTags = tuple[SolverObjective, AllocationStrategy]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll get into the meat of it: Here, AnalysisParameters is a configuration class that holds various parameters influencing the analysis process. These settings allow us to control aspects like which disasters to consider, the objectives to optimize for, and how to handle inventory and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisParameters:\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    expand_depot_set\n",
    "        Flag indicating whether inventory can be reallocated to depots that don't currently hold any stock\n",
    "    care_about_month_demand\n",
    "        Flag indicating whether we take month-by-month demand (True) or the general number (False)\n",
    "    disaster_month\n",
    "        Month from which to select disasters\n",
    "    num_months_to_average\n",
    "        Number of months to use for selecting disasters, when disasterMonth>=0\n",
    "    optimization_objectives\n",
    "        Set of objectives to use for running the optimization model\n",
    "    comparison_objectives\n",
    "        Set of objectives to use for comparing results\n",
    "    allocation_strategies\n",
    "        Which strategies to test for (re)allocation inventory to depots in the first stage\n",
    "    min_year\n",
    "        First year from which disasters should be taken into account\n",
    "    max_year\n",
    "        Last year from which disasters should be taken into account\n",
    "    scale_demand\n",
    "        Whether demand must be scaled to not exceed total available inventory or not\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    expand_depot_set: bool = False\n",
    "    care_about_month_demand: bool = True\n",
    "    disaster_month: int = -1\n",
    "    num_months_to_average: int = 3\n",
    "    optimization_objectives: list[SolverObjective] = [\n",
    "        SolverObjective.Cost,\n",
    "        SolverObjective.Time,\n",
    "    ]\n",
    "    comparison_objectives: list[SolverObjective] = list(SolverObjective)\n",
    "    allocation_strategies: list[AllocationStrategy] = list(AllocationStrategy)\n",
    "    min_year: int = 1900\n",
    "    max_year: int = 2100\n",
    "    scale_demand: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analysis class encapsulates all results and statistics from the optimization runs. It includes solutions for different objectives and strategies, along with computed metrics that help in evaluating and comparing these solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    \"\"\"\n",
    "    Analysis results for multiple optimization runs for a single dataset and item, using different objectives and allocation strategies.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    parameters:\n",
    "        Parameters used to construct the analysis\n",
    "    dataset:\n",
    "        Original dataset being analyzed\n",
    "    item:\n",
    "        Item for which the analysis was performed\n",
    "    solutions:\n",
    "        Dictionary of solutions for all solved problems\n",
    "    solution_stats\n",
    "        Index: objective, strategy\n",
    "        Columns:\n",
    "        - coveredDemandExcDummy\n",
    "        - dualTotInv\n",
    "        - totalCostIncDummy\n",
    "        - totalCostExcDummy\n",
    "        - totalDemand\n",
    "        - fractionOfDisastersUsingDummy\n",
    "        - averageUnitCost\n",
    "        - demandFulfillmentFraction\n",
    "    balance_metric\n",
    "        Index: objective\n",
    "        Columns: balanceMetric\n",
    "    units_shipped\n",
    "        Index: objective, strategy, mode\n",
    "        Columns: unitsShipped, unitsShippedWeighted\n",
    "    people_served_per_item\n",
    "        Index: objective, strategy\n",
    "        Columns: peopleServedPerItem\n",
    "    cross_ompact\n",
    "        Index: objective, strategy, other\n",
    "        Columns: impact\n",
    "    \"\"\"\n",
    "\n",
    "    parameters: AnalysisParameters\n",
    "    dataset: Dataset\n",
    "    item: Item\n",
    "    solutions: dict[SolutionTags, Solution]\n",
    "    solution_stats: pd.DataFrame\n",
    "    balance_metric: pd.DataFrame\n",
    "    units_shipped: pd.DataFrame\n",
    "    people_served_per_item: pd.DataFrame\n",
    "    cross_impact: pd.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameters: AnalysisParameters,\n",
    "        dataset: Dataset,\n",
    "        item: Item,\n",
    "        solutions: dict[SolutionTags, Solution],\n",
    "        solution_stats: pd.DataFrame,\n",
    "        balance_metric: pd.DataFrame,\n",
    "        units_shipped: pd.DataFrame,\n",
    "        people_served_per_item: pd.DataFrame,\n",
    "        cross_impact: pd.DataFrame,\n",
    "    ):\n",
    "        self.parameters = parameters\n",
    "        self.dataset = dataset\n",
    "        self.item = item\n",
    "        self.solutions = solutions\n",
    "        self.solution_stats = solution_stats\n",
    "        self.balance_metric = balance_metric\n",
    "        self.units_shipped = units_shipped\n",
    "        self.people_served_per_item = people_served_per_item\n",
    "        self.cross_impact = cross_impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AnalyzerWorker class performs the core computation. It runs optimization models for different objectives and strategies, processes the results, and computes various metrics for analysis. The _post_process method compiles these results into meaningful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzerWorker:\n",
    "    def __init__(self, parameters: AnalysisParameters):\n",
    "        self.parameters = parameters\n",
    "        self._solver = StochasticSolver()\n",
    "\n",
    "    def run(self, dataset: Dataset, item: Item) -> Analysis:\n",
    "        #a=len(str(dataset))\n",
    "        dataset = self._filter_dataset(dataset)\n",
    "        #print(len(str(dataset))!=a)\n",
    "        probabilities = {\n",
    "            disaster: 1 / len(dataset.disasters) for disaster in dataset.disasters\n",
    "        }\n",
    "\n",
    "        solutions: dict[SolutionTags, Solution] = {}\n",
    "\n",
    "        # Construct cost matrices once\n",
    "        cost_matrices = {\n",
    "            objective: self._get_cost_matrix(dataset, item, objective)\n",
    "            for objective in SolverObjective\n",
    "        }\n",
    "\n",
    "        # Construct inventory\n",
    "        inventory = self._select_inventory(dataset, item)\n",
    "        if sum(inventory.values()) == 0:\n",
    "            return None\n",
    "\n",
    "        # Construct demand\n",
    "        demand = self._select_demand(dataset, item)\n",
    "\n",
    "        # Solve models for all objectives and strategies\n",
    "        for objective in self.parameters.optimization_objectives:\n",
    "            for strategy in self.parameters.allocation_strategies:\n",
    "                problem = Problem(\n",
    "                    dataset.depots,\n",
    "                    inventory,\n",
    "                    demand,\n",
    "                    dataset.disasters,\n",
    "                    probabilities,\n",
    "                    dataset.transport_modes,\n",
    "                    cost_matrices[objective],\n",
    "                )\n",
    "                parameters = SolverParameters(strategy, self.parameters.scale_demand)\n",
    "                tags = (objective, strategy)\n",
    "                solutions[tags] = self._solver.solve(problem, parameters)\n",
    "\n",
    "        return self._post_process(dataset, item, cost_matrices, solutions)\n",
    "\n",
    "    def dispose(self):\n",
    "        self._solver.dispose()\n",
    "\n",
    "    def _select_inventory(self, dataset: Dataset, item: Item):\n",
    "        return {\n",
    "            depot: dataset.inventory.get((depot, item), 0)\n",
    "            for depot in dataset.depots\n",
    "            if self.parameters.expand_depot_set\n",
    "            or dataset.inventory.get((depot, item), 0) > 0\n",
    "        }\n",
    "\n",
    "    def _filter_dataset(self, dataset: Dataset) -> Dataset:\n",
    "        if self.parameters.disaster_month > -1:\n",
    "            months = range(\n",
    "                self.parameters.disaster_month,\n",
    "                self.parameters.disaster_month\n",
    "                + 1\n",
    "                + self.parameters.num_months_to_average,\n",
    "            )\n",
    "            months = [(month - 1) % 12 + 1 for month in months]\n",
    "            predicate: Callable[[Disaster], bool] = (\n",
    "                lambda disaster: disaster.month in months\n",
    "            )\n",
    "            dataset = dataset.take_disaster_subset(predicate)\n",
    "\n",
    "        dataset = dataset.take_disaster_subset(\n",
    "            lambda disaster: disaster.year >= self.parameters.min_year\n",
    "            and disaster.year <= self.parameters.max_year\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _select_demand(\n",
    "        self, dataset: Dataset, item: Item\n",
    "    ) -> dict[DisasterImpact, float]:\n",
    "        source = (\n",
    "            dataset.monthly_demand\n",
    "            if self.parameters.care_about_month_demand\n",
    "            else dataset.general_demand\n",
    "        )\n",
    "        return {\n",
    "            location: source.get((location, item), 0)\n",
    "            for disaster in dataset.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "        }\n",
    "\n",
    "    def _get_cost_matrix(\n",
    "        self, dataset: Dataset, item: Item, objective: SolverObjective\n",
    "    ) -> CostMatrix:\n",
    "        return {\n",
    "            key: self._get_cost_element(value, objective, item)\n",
    "            for key, value in dataset.distance.items()\n",
    "        }\n",
    "\n",
    "    def _get_cost_element(\n",
    "        self, cell: DistanceInfo, objective: SolverObjective, item: Item\n",
    "    ):\n",
    "        if objective == SolverObjective.Cost:\n",
    "            return item.weight * cell.cost_per_ton\n",
    "        elif objective == SolverObjective.Time:\n",
    "            return cell.time\n",
    "        elif objective == SolverObjective.Distance:\n",
    "            return cell.distance\n",
    "        else:\n",
    "            raise RuntimeError(f\"Undefined objective {objective}\")\n",
    "\n",
    "    def _post_process(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        item: Item,\n",
    "        costs: dict[SolverObjective, CostMatrix],\n",
    "        solutions: dict[SolutionTags, Solution],\n",
    "    ):\n",
    "        beta_source = (\n",
    "            dataset.persons_per_item_monthly\n",
    "            if self.parameters.care_about_month_demand\n",
    "            else dataset.persons_per_item_general\n",
    "        )\n",
    "        beta = {\n",
    "            location.id: beta_source[location, item]\n",
    "            for disaster in dataset.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "        }\n",
    "\n",
    "        solution_stats = pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"objective\": objective,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"coveredDemandExcDummy\": solution.covered_demand_exc_dummy,\n",
    "                    \"dualTotInv\": solution.dual_total_inventory,\n",
    "                    \"totalCostIncDummy\": solution.total_cost_inc_dummy,\n",
    "                    \"totalCostExcDummy\": solution.total_cost_exc_dummy,\n",
    "                    \"totalDemand\": solution.total_demand,\n",
    "                    \"fractionOfDisastersUsingDummy\": solution.fraction_of_disasters_using_dummy,\n",
    "                }\n",
    "                for (objective, strategy), solution in solutions.items()\n",
    "            ]\n",
    "        ).set_index([\"objective\", \"strategy\"])\n",
    "\n",
    "        df_flows = pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"objective\": objective,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"disaster\": disaster.id,\n",
    "                    \"depot\": depot.id,\n",
    "                    \"impact\": impact.id,\n",
    "                    \"location\": impact.location.id,\n",
    "                    \"mode\": mode.id,\n",
    "                    \"flow\": value,\n",
    "                    \"distance\": dataset.distance[depot, impact.location, mode].distance\n",
    "                    if depot.id != \"DUMMY\"\n",
    "                    else None,\n",
    "                }\n",
    "                for (objective, strategy), solution in solutions.items()\n",
    "                for (disaster, depot, impact, mode), value in solution.flow.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Average unit cost\n",
    "        solution_stats[\"averageUnitCost\"] = solution_stats[\"totalCostExcDummy\"] / (\n",
    "            solution_stats[\"coveredDemandExcDummy\"] + 1e-7\n",
    "        )\n",
    "\n",
    "        # Demand fulfillment fraction\n",
    "        solution_stats[\"demandFulfillmentFraction\"] = solution_stats[\n",
    "            \"coveredDemandExcDummy\"\n",
    "        ] / (solution_stats[\"totalDemand\"] + 1e-7)\n",
    "\n",
    "        # Balance metric\n",
    "        strategies = set(solution_stats.reset_index()[\"strategy\"])\n",
    "        pivoted = solution_stats.reset_index().pivot(\n",
    "            index=\"objective\", columns=\"strategy\", values=\"totalCostExcDummy\"\n",
    "        )\n",
    "        pivoted[\"balanceMetric\"] = (\n",
    "            pivoted[AllocationStrategy.MinimizeFixedInventory]\n",
    "            / (pivoted[AllocationStrategy.MinimizeTwoStage] + 1e-7)\n",
    "            if AllocationStrategy.MinimizeFixedInventory in strategies\n",
    "            and AllocationStrategy.MinimizeTwoStage in strategies\n",
    "            else None\n",
    "        )\n",
    "        balance_metric = pivoted\n",
    "\n",
    "        df_probabilities = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                disaster.id: dataset.probabilities[disaster]\n",
    "                for disaster in dataset.disasters\n",
    "            },\n",
    "            columns=[\"probability\"],\n",
    "            orient=\"index\",\n",
    "        )\n",
    "\n",
    "        # Units shipped\n",
    "        df_flow_no_dummy = df_flows.join(df_probabilities, on=\"disaster\")\n",
    "        df_flow_no_dummy = df_flow_no_dummy[\n",
    "            df_flow_no_dummy[\"depot\"] != \"DUMMY\"\n",
    "        ]  # TODO Replace hardcoded dummy ID\n",
    "        temp = df_flow_no_dummy.copy()\n",
    "        temp[\"unitsShipped\"] = temp[\"probability\"] * temp[\"flow\"]\n",
    "        temp[\"unitsShippedWeighted\"] = temp[\"unitsShipped\"] * temp[\"distance\"]\n",
    "        units_shipped = (\n",
    "            temp.set_index([\"objective\", \"strategy\", \"mode\"])[\n",
    "                [\"unitsShipped\", \"unitsShippedWeighted\"]\n",
    "            ]\n",
    "            .groupby([\"objective\", \"strategy\", \"mode\"])\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        # People served per item\n",
    "        temp = df_flow_no_dummy.copy()\n",
    "        temp[\"beta\"] = temp[\"impact\"].apply(lambda loc: beta[loc])\n",
    "        temp[\"peopleServed\"] = temp[\"probability\"] * temp[\"beta\"] * temp[\"flow\"]\n",
    "        people_served = (\n",
    "            temp.set_index([\"objective\", \"strategy\"])[\"peopleServed\"]\n",
    "            .groupby([\"objective\", \"strategy\"])\n",
    "            .sum()\n",
    "        )\n",
    "        people_served_per_item = pd.DataFrame(\n",
    "            people_served / (solution_stats[\"coveredDemandExcDummy\"] + 1e-7),\n",
    "            columns=[\"peopleServedPerItem\"],\n",
    "        )\n",
    "\n",
    "        # Impact of optimizing one objective on another objective\n",
    "        impact = []\n",
    "        for other in self.parameters.comparison_objectives:\n",
    "            cost = {\n",
    "                (depot.id, location.id, mode.id): value\n",
    "                for (depot, location, mode), value in costs[other].items()\n",
    "            }\n",
    "            temp = df_flow_no_dummy.copy()\n",
    "            if temp.empty:\n",
    "                raise RuntimeError(\"Empty flow matrix encountered\")\n",
    "            temp[\"cost\"] = temp.apply(\n",
    "                lambda row: cost[row[\"depot\"], row[\"location\"], row[\"mode\"]], axis=1\n",
    "            )\n",
    "            temp[\"other\"] = other\n",
    "            temp[\"impact\"] = temp[\"cost\"] * temp[\"probability\"] * temp[\"flow\"]\n",
    "            impact.append(temp.reset_index())\n",
    "        cross_impact = (\n",
    "            pd.concat(impact)\n",
    "            .groupby([\"objective\", \"strategy\", \"other\"])[[\"impact\"]]\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        return Analysis(\n",
    "            self.parameters,\n",
    "            dataset,\n",
    "            item,\n",
    "            solutions,\n",
    "            solution_stats,\n",
    "            balance_metric,\n",
    "            units_shipped,\n",
    "            people_served_per_item,\n",
    "            cross_impact,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analyzer class manages the analysis workflow. It can run analysis for a single item or for all items across various inventory scenarios. It utilizes the AnalyzerWorker to perform the computations and collects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    \"\"\"\n",
    "    Service responsible for performing optimization runs and analysis on the results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters: AnalysisParameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def run(self, dataset: Dataset, item: Item) -> Analysis:\n",
    "        worker = AnalyzerWorker(self.parameters)\n",
    "        result = worker.run(dataset, item)\n",
    "        worker.dispose()\n",
    "        return result\n",
    "\n",
    "    def run_all(self, dataset: Dataset) -> dict[tuple[str, Item], Analysis]:\n",
    "        inventory_datasets = {\n",
    "            filename: dataset.take_inventory_scenario(filename)\n",
    "            for filename in dataset.inventory_scenarios\n",
    "        }\n",
    "        tasks = [\n",
    "            (filename, inventory_dataset, item)\n",
    "            for (filename, inventory_dataset) in inventory_datasets.items()\n",
    "            for item in inventory_dataset.items\n",
    "        ]\n",
    "\n",
    "        return self._run_tasks(tasks)\n",
    "\n",
    "    def _run_tasks(self, tasks: list[tuple[str, Dataset, Item]]):\n",
    "        use_multi_processing = getenv(\"CI\", \"false\") == \"false\"\n",
    "        use_multi_processing = False\n",
    "        if use_multi_processing:\n",
    "            with Pool(\n",
    "                initializer=_analysis_worker_init, initargs=[self.parameters]\n",
    "            ) as pool:\n",
    "                result: list[tuple[str, Analysis]] = pool.map(\n",
    "                    _analysis_worker_call, tasks\n",
    "                )\n",
    "        else:\n",
    "            worker = AnalyzerWorker(self.parameters)\n",
    "            result = [\n",
    "                (filename, worker.run(dataset, item))\n",
    "                for (filename, dataset, item) in tasks\n",
    "            ]\n",
    "            worker.dispose()\n",
    "        return {\n",
    "            (filename, analysis.item): analysis\n",
    "            for (filename, analysis) in result\n",
    "            if analysis is not None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two functiomns function initializes a global worker for use in multiprocessing scenarios and calls the run method of the global worker with the provided arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analysis_worker_init(parameters):\n",
    "    global worker\n",
    "    worker = AnalyzerWorker(parameters)\n",
    "\n",
    "\n",
    "def _analysis_worker_call(arg: tuple[Dataset, Item]) -> tuple[str, Analysis]:\n",
    "    global worker\n",
    "    (filename, dataset, item) = arg\n",
    "    return (filename, worker.run(dataset, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting it ready for Optimization\n",
    "\n",
    "\n",
    "Let's take a look at how the data is formatted to be used in our solver. First we will set the context of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the Prevalent Use of Dictionaries vs Lists?\n",
    "Dictionaries are often favored over lists in production code for several reasons:\n",
    "\n",
    "-\tKey-Value Pairs: Dictionaries provide a convenient way to store and retrieve data by keys rather than by index. This makes dictionaries especially useful for scenarios where data is logically organized by unique identifiers (e.g., JSON-like structures, configurations).\n",
    "-\tReadability and Maintainability: With dictionaries, code becomes more readable and self-explanatory. For example, user['name'] is more descriptive than user[0].\n",
    "-\tFlexibility in Data Manipulation: Unlike lists, dictionaries allow quick updates and deletions by key, improving flexibility when modifying data.\n",
    "-\tPerformance: For lookups, inserts, and deletions, dictionaries (hash maps) often provide average O(1) time complexity, while lists require O(n) for searching or removing elements unless the position is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB, tupledict\n",
    "\n",
    "from src.data import Depot, Disaster, DisasterImpact, Location, TransportMode\n",
    "\n",
    "\n",
    "class AllocationStrategy(IntEnum):\n",
    "    '''\n",
    "    The AllocationStrategy enumeration defines different strategies for allocating inventory in \n",
    "    our optimization model\n",
    "\n",
    "    MinimizeTwoStage: Allows inventory reallocation in both stages of the model.\n",
    "\n",
    "    MinimizeFixedInventory: Keeps the inventory fixed as in the initial state.\n",
    "\n",
    "    WorstDepot: Allocates all inventory to the worst-performing depot to test the model's robustness.\n",
    "    '''\n",
    "    MinimizeTwoStage = 0\n",
    "    MinimizeFixedInventory = 1\n",
    "    WorstDepot = 2\n",
    "\n",
    "'''\n",
    "The SolverParameters class holds configuration settings for the solver.\n",
    "'''\n",
    "@dataclass(frozen=True)\n",
    "class SolverParameters:\n",
    "    \"\"\"\n",
    "    Parameters that influence how the solver transforms the Problem into a mathematical model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    allocation_strategy\n",
    "        If and how inventory can be reallocated\n",
    "    scale_demand\n",
    "        Whether demand should be scaled (down) to not exceed supply\n",
    "    \"\"\"\n",
    "\n",
    "    allocation_strategy: AllocationStrategy = (\n",
    "        AllocationStrategy.MinimizeFixedInventory,\n",
    "    )\n",
    "    scale_demand: bool = False\n",
    "\n",
    "'''\n",
    "We define a type alias CostMatrix for readability. It represents a dictionary mapping a \n",
    "tuple of (source location, target location, transport mode) to a cost value (e.g., transportation \n",
    "cost, time, or distance). This matrix is essential for the optimization model to calculate \n",
    "costs associated with moving goods from depots to disaster impact locations.\n",
    "'''\n",
    "CostMatrix = dict[Tuple[Location, Location, TransportMode], float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Problem:\n",
    "    '''\n",
    "    The Problem class encapsulates all the data required to define an optimization problem:\n",
    "\n",
    "    depots: A list of depots where inventory is stored.\n",
    "    \n",
    "    inventory: A dictionary mapping each depot to the quantity of inventory it holds.\n",
    "    \n",
    "    demand: A dictionary mapping each DisasterImpact location to its demand.\n",
    "    \n",
    "    disasters: A list of disasters being considered.\n",
    "    \n",
    "    probabilities: A dictionary mapping each disaster to its probability of occurrence.\n",
    "    \n",
    "    transport_modes: A list of available transport modes.\n",
    "    \n",
    "    cost: The CostMatrix containing cost information for transporting goods.*\n",
    "    '''\n",
    "    depots: list[Depot]\n",
    "    inventory: dict[Depot, int]\n",
    "    demand: dict[DisasterImpact, float]\n",
    "    disasters: list[Disaster]\n",
    "    probabilities: dict[Disaster, float]\n",
    "    transport_modes: list[TransportMode]\n",
    "    cost: CostMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Solution class stores the results obtained after solving a Problem with specific SolverParameters. It includes various metrics and variables that describe the performance and decisions made by the optimization model, such as total costs, demand coverage, dual variables, and optimal flows. The _dummy_depot is a special depot used internally to handle unmet demand in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Solution:\n",
    "    \"\"\"\n",
    "    Result of solving a single Problem with specific SolverParameters\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    total_cost_inc_dummy\n",
    "        Total transportation cost including artificial cost for using the dummy node (myObj)\n",
    "    total_cost_exc_dummy\n",
    "        Total transportation cost from the real depots (myObjNoDum)\n",
    "    total_demand\n",
    "        Total demand in the input data (myWeightedDemand)\n",
    "    covered_demand_exc_dummy\n",
    "        Demand served from real depots, averaged over all scenarios (myWeightedDemandMetNoDum)\n",
    "    fraction_of_disasters_using_dummy\n",
    "        Fraction of disaster scenarios for which not enough real inventory is available (myFractionOfDisastersUsingDummy)\n",
    "    duals_inventory_exc_dummy_plus_dummy_cost\n",
    "        Adjusted dual variables for the inventory constraints (values are independent of dummy costs) (dualsInvNoDum_PlusDummyCost)\n",
    "    duals_inventory_exc_dummy_unadjusted\n",
    "        Original dual variables for the inventory constraints, aggregated over disaster scenarios (dualsInvNoDum_UnAdj)\n",
    "    duals_inventory_exc_dummy_all\n",
    "        All original dual variables for the inventory constraints (dualsInvNoDum_All)\n",
    "    flow_exc_dummy\n",
    "        Allocation of depot inventory to disaster locations in each scenario, excluding the dummy depot (myFlowNoDum)\n",
    "    flow\n",
    "        Allocation of depot inventory to disaster locations in each scenario (myFlow)\n",
    "    optimal_inventory\n",
    "        Optimal or fixed allocation of inventory to depots (myOptInvNoDum)\n",
    "    dual_total_inventory\n",
    "        Dual variable for the total inventory constraint (dualTotInv)\n",
    "    \"\"\"\n",
    "\n",
    "    total_cost_inc_dummy: float\n",
    "    total_cost_exc_dummy: float\n",
    "    total_demand: float\n",
    "    covered_demand_exc_dummy: float\n",
    "    fraction_of_disasters_using_dummy: float\n",
    "    duals_inventory_exc_dummy_plus_dummy_cost: dict[Depot, float]\n",
    "    duals_inventory_exc_dummy_unadjusted: dict[Depot, float]\n",
    "    duals_inventory_exc_dummy_all: dict[Tuple[Disaster, Depot], float]\n",
    "    flow_exc_dummy: dict[Tuple[Disaster, Depot, DisasterImpact, TransportMode], float]\n",
    "    flow: dict[Tuple[Disaster, Depot, DisasterImpact, TransportMode], float]\n",
    "    optimal_inventory: dict[Depot, float]\n",
    "    dual_total_inventory: float\n",
    "\n",
    "    _dummy_depot: Depot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StochasticSolver class encapsulates the logic for solving stochastic optimization problems. The class variables _threshold_cost_elim and _threshold_cost_dummy are large constants used to effectively eliminate certain arcs or penalize the use of the dummy depot in the optimization model. The constructor initializes a dummy location and creates a Gurobi environment with specific parameters (e.g., disabling output and setting single-threaded execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticSolver:\n",
    "    _threshold_cost_elim: float = 1e9\n",
    "    _threshold_cost_dummy: float = 1e9\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dummy = Location(\"DUMMY\", \"\", \"\", 0, 0)\n",
    "        self._env = gp.Env(params={\"OutputFlag\": 0, \"Threads\": 1})\n",
    "    '''\n",
    "    The dispose method releases resources associated with the Gurobi environment. \n",
    "    It's important to call this method after solving problems to prevent resource \n",
    "    leaks, especially when solving multiple problems in a loop.\n",
    "    '''\n",
    "    def dispose(self):\n",
    "        self._env.dispose()\n",
    "        self._env = None\n",
    "\n",
    "    '''\n",
    "    In the solve method, we begin by preparing the data for the optimization model:\n",
    "    \n",
    "    We include both real depots and the dummy depot in our list of sources.\n",
    "    \n",
    "    Demand scaling is performed if specified in the parameters.\n",
    "    \n",
    "    We construct the list of possible arcs in the transportation network, filtering out those with \n",
    "    prohibitive costs unless they involve the dummy depot.\n",
    "    \n",
    "    We calculate the cost for each arc, adjusting for disaster probabilities to reflect the expected\n",
    "    cost across scenarios.\n",
    "    \n",
    "    We initialize a Gurobi model named \"StochLP\" for solving the stochastic linear program\n",
    "    '''\n",
    "\n",
    "    def solve(self, problem: Problem, parameters: SolverParameters) -> Solution:\n",
    "        sources = problem.depots + [self._dummy]\n",
    "\n",
    "        demand = (\n",
    "            self._scale_demand(problem) if parameters.scale_demand else problem.demand\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        The arcs represent the possible transportation routes from the supply sources (depots and a dummy depot)\n",
    "        to the disaster-impacted locations for each disaster scenario and transport mode. Specifically, \n",
    "        each arc is a tuple (k, i, j, v) where:\n",
    "\n",
    "        k i a disaster scenario from the list of disasters being considered.\n",
    "        \n",
    "        i is a source location, which can be a real depot or a special dummy depot used to model unmet demand.\n",
    "        \n",
    "        j is a disaster impact location, representing an area affected by disaster k where relief goods are needed.\n",
    "        \n",
    "        v is a transport mode, such as road, air, or sea transport.\n",
    "        These arcs form the edges of the transportation network in the model. They define all feasible routes along \n",
    "        which relief items can be transported from depots to impacted locations under different disaster scenarios and transportation options.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        arcs = gp.tuplelist(\n",
    "            [\n",
    "                (k, i, j, v)\n",
    "                for i in sources\n",
    "                for k in problem.disasters\n",
    "                for j in k.impacted_locations\n",
    "                for v in problem.transport_modes\n",
    "                if (\n",
    "                    self._get_arc_cost(problem.cost, i, j, v)\n",
    "                    < self._threshold_cost_elim\n",
    "                )\n",
    "                or (i == self._dummy)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        arc_cost = {\n",
    "            (k, i, j, v): self._get_arc_cost(problem.cost, i, j, v)\n",
    "            * problem.probabilities[k]\n",
    "            for (k, i, j, v) in arcs\n",
    "        }\n",
    "\n",
    "        model = gp.Model(\"StochLP\", env=self._env)\n",
    "\n",
    "        # First stage variable: Quantity to be allocated to each depot\n",
    "        x: tupledict[Depot, gp.Var] = model.addVars(problem.depots, lb=0, name=\"x\")\n",
    "\n",
    "        # Second stage variable: Quantity transported from (real or dummy) depot to disaster locations using each mode of transport\n",
    "        y: tupledict[\n",
    "            Tuple[Disaster, Union[Depot, Location], DisasterImpact, TransportMode],\n",
    "            gp.Var,\n",
    "        ] = model.addVars(arcs, lb=0, obj=arc_cost, name=\"y\")\n",
    "\n",
    "        # Constraint: Total incoming arc flow must cover demand for each disaster location\n",
    "        model.addConstrs(\n",
    "            (\n",
    "                y.sum(k, \"*\", j, \"*\") == demand[j]\n",
    "                for k in problem.disasters\n",
    "                for j in k.impacted_locations\n",
    "            ),\n",
    "            name=\"satisfyDemand\",\n",
    "        )\n",
    "\n",
    "        # Constraint: Total outgoing arc flow must match initial or reallocated inventory\n",
    "        inventory_balance: tupledict[\n",
    "            Tuple[Disaster, Depot], gp.Constr\n",
    "        ] = model.addConstrs(\n",
    "            (\n",
    "                y.sum(k, i, \"*\", \"*\") <= x[i]\n",
    "                for k in problem.disasters\n",
    "                for i in problem.depots\n",
    "            ),\n",
    "            name=\"satisfySupply\",\n",
    "        )\n",
    "\n",
    "        # Constraint: Ensure inventory reallocation matches total existing inventory\n",
    "        total_initial_inventory = sum(problem.inventory.values())\n",
    "        match_total_inventory = model.addConstr(x.sum() == total_initial_inventory)\n",
    "\n",
    "        '''\n",
    "        We define a helper function fix_inventory_balance to fix the inventory variables (x) to specific values, \n",
    "        effectively turning them into constants in the model. Based on the allocation_strategy\n",
    "        \n",
    "        MinimizeFixedInventory: We fix the inventory variables to their initial values, preventing reallocation.\n",
    "        \n",
    "        WorstDepot: We iterate over all depots, allocating all inventory to each one individually to find the depot \n",
    "        that results in the worst (highest) objective value. We then fix the inventory allocation to that depot, \n",
    "        which is useful for stress-testing the model's performance under adverse conditions.\n",
    "        '''\n",
    "        def fix_inventory_balance(values: dict[Disaster, float]):\n",
    "            for key, value in values.items():\n",
    "                x[key].LB = x[key].UB = value\n",
    "\n",
    "        if parameters.allocation_strategy == AllocationStrategy.MinimizeFixedInventory:\n",
    "            fix_inventory_balance(problem.inventory)\n",
    "        elif parameters.allocation_strategy == AllocationStrategy.WorstDepot:\n",
    "            worst_depot = None\n",
    "            worst_objective = -1e100\n",
    "            for depot in problem.depots:\n",
    "                centralized_inventory = {\n",
    "                    other: total_initial_inventory if other == depot else 0\n",
    "                    for other in problem.depots\n",
    "                }\n",
    "                fix_inventory_balance(centralized_inventory)\n",
    "                model.optimize()\n",
    "                if model.Status != GRB.Status.OPTIMAL:\n",
    "                    raise RuntimeError(\"Could not solve model to optimality\")\n",
    "                if model.ObjVal > worst_objective:\n",
    "                    worst_depot = depot\n",
    "                    worst_objective = model.ObjVal\n",
    "            centralized_inventory = {\n",
    "                other: total_initial_inventory if other == worst_depot else 0\n",
    "                for other in problem.depots\n",
    "            }\n",
    "            fix_inventory_balance(centralized_inventory)\n",
    "\n",
    "        model.optimize()\n",
    "        if model.Status != GRB.Status.OPTIMAL:\n",
    "            raise RuntimeError(\"Could not solve model to optimality\")\n",
    "\n",
    "        # Total transport cost\n",
    "        total_cost_inc_dummy = model.ObjVal\n",
    "        dummy_cost = sum(\n",
    "            var.X * var.Obj for var in y.select(\"*\", self._dummy, \"*\", \"*\")\n",
    "        )\n",
    "        total_cost_exc_dummy = total_cost_inc_dummy - dummy_cost\n",
    "\n",
    "        # Demand met without using the dummy node\n",
    "        covered_demand_by_dummy = sum(\n",
    "            y[k, i, j, v].X * problem.probabilities[k]\n",
    "            for (k, i, j, v) in arcs.select(\"*\", self._dummy, \"*\", \"*\")\n",
    "        )\n",
    "        total_demand = sum(\n",
    "            local_demand * problem.probabilities[j.disaster]\n",
    "            for (j, local_demand) in demand.items()\n",
    "        )\n",
    "        covered_demand_exc_dummy = total_demand - covered_demand_by_dummy\n",
    "\n",
    "        # Flow in solution\n",
    "        solution_y = {key: y[key].X for key in arcs}\n",
    "\n",
    "        # Fraction of disaster scenarios in which the dummy supply is used\n",
    "        fraction_of_disasters_using_dummy = len(\n",
    "            [\n",
    "                disaster\n",
    "                for disaster in problem.disasters\n",
    "                if sum(\n",
    "                    solution_y[key]\n",
    "                    for key in arcs.select(disaster, self._dummy, \"*\", \"*\")\n",
    "                )\n",
    "                > 0\n",
    "            ]\n",
    "        ) / len(problem.disasters)\n",
    "\n",
    "        # Dual variables for the inventory balance constraints\n",
    "        dual_correction = fraction_of_disasters_using_dummy * self._threshold_cost_dummy\n",
    "        duals_inventory_exc_dummy_unadjusted = {\n",
    "            i: sum(inventory_balance[k, i].Pi for k in problem.disasters)\n",
    "            for i in problem.depots\n",
    "        }\n",
    "        duals_inventory_exc_dummy_plus_dummy_cost = {\n",
    "            i: dual_correction + pi\n",
    "            for (i, pi) in duals_inventory_exc_dummy_unadjusted.items()\n",
    "        }\n",
    "        duals_inventory_exc_dummy_all = {\n",
    "            (k, i): constr.Pi for (k, i), constr in inventory_balance.items()\n",
    "        }\n",
    "\n",
    "        flow = {(k, i, j, v): var.X for (k, i, j, v), var in y.items() if var.X > 0}\n",
    "\n",
    "        flow_exc_dummy = {\n",
    "            (k, i, j, v): value\n",
    "            for (k, i, j, v), value in flow.items()\n",
    "            if i != self._dummy\n",
    "        }\n",
    "\n",
    "        optimal_inventory = {depot: var.X for depot, var in x.items()}\n",
    "\n",
    "        dual_total_inventory = (\n",
    "            match_total_inventory.Pi\n",
    "            if parameters.allocation_strategy\n",
    "            == AllocationStrategy.MinimizeFixedInventory\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        Finally, we package all the extracted metrics and variables into a Solution object and return it. This object \n",
    "        provides a comprehensive view of the optimization results, enabling further analysis or reporting.\n",
    "        '''\n",
    "\n",
    "        return Solution(\n",
    "            total_cost_inc_dummy,\n",
    "            total_cost_exc_dummy,\n",
    "            total_demand,\n",
    "            covered_demand_exc_dummy,\n",
    "            fraction_of_disasters_using_dummy,\n",
    "            duals_inventory_exc_dummy_plus_dummy_cost,\n",
    "            duals_inventory_exc_dummy_unadjusted,\n",
    "            duals_inventory_exc_dummy_all,\n",
    "            flow_exc_dummy,\n",
    "            flow,\n",
    "            optimal_inventory,\n",
    "            dual_total_inventory,\n",
    "            self._dummy,\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    The _get_arc_cost method retrieves the cost of transporting goods from a source to a target using a specific transport mode. If the \n",
    "    source is the dummy depot, it returns a high dummy cost to penalize \n",
    "    its use. If there is no cost information available for a given arc \n",
    "    (i.e., cell is None), it returns a high elimination threshold cost \n",
    "    to effectively remove that arc from consideration.\n",
    "    '''\n",
    "\n",
    "    def _get_arc_cost(\n",
    "        self,\n",
    "        cost: CostMatrix,\n",
    "        source: Location,\n",
    "        target: DisasterImpact,\n",
    "        mode: TransportMode,\n",
    "    ):\n",
    "        if source == self._dummy:\n",
    "            return self._threshold_cost_dummy\n",
    "        cell = cost.get((source, target.location, mode))\n",
    "        return self._threshold_cost_elim if cell == None else cell\n",
    "\n",
    "    '''\n",
    "    The _scale_demand method adjusts the demand levels so that total demand \n",
    "    does not exceed total supply. For each disaster, it calculates a scaling \n",
    "    factor based on the ratio of total supply to total demand. It then applies \n",
    "    this factor to the demand at each impacted location. This is useful in scenarios \n",
    "    where supply constraints are tight, ensuring that the optimization model \n",
    "    operates within feasible limits.\n",
    "    \n",
    "    '''\n",
    "    def _scale_demand(self, problem: Problem) -> dict[DisasterImpact, float]:\n",
    "        supply = sum(problem.inventory.values())\n",
    "        result: dict[DisasterImpact, float] = {}\n",
    "        for disaster in problem.disasters:\n",
    "            total_demand = sum(\n",
    "                problem.demand[location] for location in disaster.impacted_locations\n",
    "            )\n",
    "            factor = min(1, supply / total_demand) if total_demand > 1e-6 else 1\n",
    "            for location in disaster.impacted_locations:\n",
    "                result[location] = factor * problem.demand[location]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After diving into the code, it’s clear that it’s exceptionally well-crafted, following best practices and maintaining high readability. The lead developer shared some key insights that contributed to this level of quality. These takeaways not only explain how the code reached its current state, but also offer valuable lessons for future projects. Let’s explore these insights to understand what makes the code stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Commenting on Different Projects with Different People Involved**\n",
    "\n",
    "\tWhen working on different projects involving different teams, it is crucial to adapt your commenting style to meet the team’s specific needs and coding standards. Here are some tips for ensuring effective commenting styles across projects:\n",
    "\n",
    "\t- Understand Team Preferences: Some teams prefer more verbose comments to ensure all members understand the code, while others might lean towards minimal commenting to keep code clean. Spend time understanding each team’s culture and coding standards.\n",
    "\n",
    "\t\n",
    "\t- Project-specific Documentation: Tailor the level and style of commenting to fit the project’s needs. For example, a data science team might require detailed explanations of algorithms and mathematical steps, while a software engineering team may focus more on architectural and functional documentation.\n",
    "\t-\tReusable Templates: Create reusable templates or guidelines for comments that can be adapted easily for each project. For instance, standardize on docstring formats (e.g., Google-style, reStructuredText) and comment structures that can be tweaked as per the project’s requirements.\n",
    "\t-\tConsistency Is Key: Regardless of the project, maintain consistency in your commenting style within a single codebase. This reduces cognitive load and makes it easier for team members to understand the code quickly.\n",
    "\t-\tCommunicate Clearly: Have an open discussion with your teammates about their preferences. Clearly defined comment guidelines should be set and documented at the beginning of each project.\n",
    "\n",
    "2. **Why the Prevalent Use of Dictionaries vs Lists?**\n",
    "\tIn the provided code examples, dictionaries are extensively used over lists, and this choice offers several advantages:\n",
    "\n",
    "\t-\t**Key-Value Access**: Dictionaries allow for fast retrieval of data using keys, which is essential when dealing with entities identified by unique attributes. For example, the Dataset class uses dictionaries like inventory, probabilities, and distance to map complex relationships between depots, disasters, and locations. \n",
    "\t\t```python\n",
    "\t\tinventory: dict[Tuple[Depot, Item], int]\n",
    "\t\tDistanceMatrix: Dict[Tuple[Location, Location, TransportMode], DistanceInfo]\n",
    "\t\t```\n",
    "\n",
    "\t-   **Readability**: Using dictionaries makes the code more self-explanatory. Accessing ```inventory[(depot, item)]``` is clearer than using index-based access in a list, improving code readability.\n",
    "\n",
    "\t-\t**Performance**: Dictionaries provide average O(1) time complexity for lookups, which is beneficial when working with large datasets, as is common in disaster logistics modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Testing and Debugging**\n",
    "\n",
    "\tTesting and debugging are crucial for ensuring the reliability of production code, especially in complex systems like our stochastic solver where very real-world impacts can occur if bugs go unnoticed. We haven’t shown them specifically here for the sake of brevity; however, let’s take a look at how we change our code to accommodate them:\n",
    "\n",
    "\t-\t**Error Handling**: In the StochasticSolver class, we include checks after optimization to ensure that the model has reached an optimal solution. If not, we raise a RuntimeError with a clear message. For example:\n",
    "\n",
    "\t\t```python\n",
    "\t\tmodel.optimize()\n",
    "\t\tif model.Status != GRB.Status.OPTIMAL:\n",
    "\t\t\traise RuntimeError(\"Could not solve model to optimality\")\n",
    "\t\t```\n",
    "\t-\t**Modularity for Testing**: The code is organized into modular functions and methods, such as _get_arc_cost and _scale_demand, which makes unit testing more manageable. Each function can be tested independently to verify that it behaves as expected.\n",
    "\n",
    "\t\t-\t**Unit Tests**: Write unit tests for every function or module to ensure that each part works as expected independently. Unit tests should be automated and run frequently to catch errors early.\n",
    "\t\t-\t**Integration Tests**: Test interactions between different modules to catch integration-related issues. Use mocking or stubbing to isolate dependencies when needed.\n",
    "\t\n",
    "\t\t-\t**Logging**: Make effective use of logging with different levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). Logging provides a way to understand what the code was doing at the time an error occurred without having to run it in a debugging mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Any Particular Naming Rules? Discuss with Others**\n",
    "\n",
    "\tConsistent and meaningful naming conventions are crucial for collaboration:\n",
    "\n",
    "\t-\t**CamelCase vs. snake_case**: Choose a naming convention that fits the project, and stick with it throughout the codebase. In this case we have:\n",
    "\t\t- CamelCase for classes like ```DisasterImpact```, ```AnalyzerWorker```, and ```StochasticSolver```\n",
    "\t\t-\tsnake_case for variables and functions like ```total_cost_inc_dummy```, ```_filter_dataset```, and ```optimal_inventory``` ) \n",
    "\n",
    "\t-\t**Descriptive Names**: Use descriptive names that clearly indicate the purpose of a variable, function, or class. Avoid abbreviations unless they are standard and well-known.\n",
    "\t\n",
    "\t-\t**Prefixed Naming**: When needed, prefix variables to indicate their type or usage (is_ for booleans, num_ for numbers, str_ for strings). This is more common in languages with weaker type systems.\n",
    "\n",
    "\t\n",
    "\t-\t**Team Agreements**: Have a discussion with your team to establish a naming convention that everyone agrees on. Document these conventions in a style guide that is accessible to all team members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. **Error Handling**\n",
    "\n",
    "\tProper error handling is vital to ensure robustness in production code:\n",
    "\t-\t**Specific Exceptions**: The code raises specific exceptions with informative messages. For example, in the ```take_inventory_scenario``` method of the Dataset class, a RuntimeError is raised if an inventory scenario is not found\n",
    "\t\t```python\n",
    "\t\tif filename not in self.inventory_scenarios:\n",
    "\t\traise RuntimeError(\"Inventory scenario not found\")\n",
    "\t\t```\n",
    "\t-\t**Fallback Mechanisms**: In the solver, when a solution is not optimal, the code doesn't proceed blindly but instead stops execution, which prevents cascading errors and makes debugging easier.\n",
    "\t-\t**Validation Checks**: Before performing operations, the code often checks for potential issues, such as verifying that the total inventory is not zero before proceeding with the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. **Global Variables**\n",
    "\n",
    "\tGlobal variables should generally be avoided in production code due to potential issues with maintainability and concurrency:\n",
    "\t-\t**Encapsulation**: Encapsulate variables within classes or functions to limit their scope and prevent unintended modifications. Classes like ```AnalyzerWorker``` and ```StochasticSolver``` encapsulate their data and methods, preventing external interference.\n",
    "\t-\t**Controlled Scope**: In multiprocessing scenarios, global variables are used judiciously. For instance, a global worker is initialized in the ```_analysis_worker_init``` function for use in worker processes, but its scope is limited and managed carefully.\n",
    "\t\t```python\n",
    "\t\tdef _analysis_worker_init(parameters):\n",
    "\t\t\tglobal worker\n",
    "\t\t\tworker = AnalyzerWorker(parameters)\n",
    "\n",
    "\t\t```\n",
    "\t-\t**Unintended Side Effects**: Global variables can lead to unexpected side effects, making the code harder to debug and maintain.\n",
    "\t-\t**Thread Safety**: In multi-threaded applications, global variables can lead to race conditions if not handled properly.\n",
    "\t-\t**Use Constants Judiciously**: If you must use global variables, restrict them to constants (immutable values) and use them sparingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "7. **Typing**\n",
    "\n",
    "\tStrong typing adds robustness and clarity to production code:\n",
    "\n",
    "\t-\t**Function Annotations**: Methods specify the types of their arguments and return values, which aids in understanding what kinds of data are expected.\n",
    "\t\t```python\n",
    "\t\tdef _get_cost_element(\n",
    "\t\t\tself, cell: DistanceInfo, objective: SolverObjective, item: Item):\n",
    "\t\t```\n",
    "\n",
    "\n",
    "\t-\t**Type Aliases**: Defining type aliases like CostMatrix and DistanceMatrix makes complex types more readable and maintainable.\n",
    "\t\t```python\n",
    "\t\tCostMatrix = dict[Tuple[Location, Location, TransportMode], float]\n",
    "\t\t```\n",
    "\t-\t**Static Type Checking**: Using type hints allows tools like mypy to perform static type checking, catching potential type errors before runtime.\n",
    "\n",
    "\n",
    "\n",
    "By following these principles, you can improve code quality, collaboration, and maintainability in a production environment, while also tailoring your practices to different projects and teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it All\n",
    "\n",
    "So what would running all this look like? Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = \"vanuatu_simple\"\n",
    "\n",
    "# Run optimization\n",
    "reader = CsvProblemReader()\n",
    "dataset = reader.read(DATA_DIR / COUNTRY)\n",
    "\n",
    "parameters = AnalysisParameters()\n",
    "analyzer = Analyzer(parameters)\n",
    "result = analyzer.run_all(dataset)\n",
    "\n",
    "exisiting_stock_df = calc_exisiting_stock_df(dataset, country=COUNTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_DIR = Path.cwd().resolve()\n",
    "DATA_DIR = WORKSPACE_DIR / \"data\"\n",
    "TEST_DATA_DIR = WORKSPACE_DIR / \"data\" / \"test_data\"\n",
    "DASHBOARD_OUTPUT_PATH = WORKSPACE_DIR / \"dashboard_output\"\n",
    "\n",
    "priority_change_df = create_priority_change(result, country=COUNTRY)\n",
    "\n",
    "all_duals_df = pd.DataFrame()\n",
    "for key, value in result.items():\n",
    "    scenario = get_scenario(key)\n",
    "\n",
    "    if scenario != \"actual\":\n",
    "        continue\n",
    "\n",
    "    duals_df = calc_duals_by_warehouse(value)\n",
    "    all_duals_df = pd.concat([all_duals_df, duals_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics():\n",
    "    ### ProvinceAssessDF dashboard file\n",
    "    provinces_df = pd.read_csv(DATA_DIR / COUNTRY / \"province_lookup.csv\")\n",
    "    provinces_df: pd.DataFrame = ProvinceLookupDF.validate(provinces_df)\n",
    "    province_assess_df: pd.DataFrame = create_province_assess_df(\n",
    "        all_duals_df,\n",
    "        provinces_df,\n",
    "        COUNTRY,\n",
    "    )\n",
    "\n",
    "    ### WhStockAssessDF dashboard file\n",
    "    duals_df_copy = all_duals_df.copy()\n",
    "    wh_stock_assess: pd.DataFrame = item_stock_assess(duals_df_copy, provinces_df, COUNTRY)\n",
    "\n",
    "\n",
    "    ### Reallocation dashboard file\n",
    "    reallocation_df, single_warehouse_df = reallocation_dashboard_files(\n",
    "        wh_stock_assess.copy(),\n",
    "        result,\n",
    "        COUNTRY,\n",
    "        wh_stock_assess=wh_stock_assess,\n",
    "    )\n",
    "\n",
    "    ### Disaster totals dashboard file\n",
    "    dis_totals_df = create_disaster_totals(dataset, COUNTRY)\n",
    "\n",
    "\n",
    "    ### Save dashboard files\n",
    "\n",
    "    if not (DASHBOARD_OUTPUT_PATH / COUNTRY).exists():\n",
    "        (DASHBOARD_OUTPUT_PATH / COUNTRY).mkdir(parents=True)\n",
    "\n",
    "    exisiting_stock_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"exisiting_stock.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    priority_change_df = priority_change_df.loc[\n",
    "        priority_change_df[BalMetricsDashboard.run_pct] == \"actual\"\n",
    "    ]\n",
    "    priority_change_df = priority_change_df.drop(columns=[BalMetricsDashboard.run_pct])\n",
    "    priority_change_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"priority_change.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    province_assess_df = province_assess_df.drop(columns=[ProvinceAssessDF.time])\n",
    "    province_assess_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"province_assess.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    dec_wh_stock_assess = wh_stock_assess.drop(columns=[ItemProvinceAssessDF.time_hms])\n",
    "    dec_wh_stock_assess.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"wh_stock_assess_as_decimal.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    wh_stock_assess = wh_stock_assess.drop(columns=[ItemProvinceAssessDF.time_hms])\n",
    "    wh_stock_assess.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"wh_stock_assess.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    reallocation_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"reallocation.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    single_warehouse_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"single_warehouse.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    dis_totals_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"disaster_totals.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurobi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
